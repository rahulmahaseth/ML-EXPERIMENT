{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# amazon_food_review_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk# natural language tool kit\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from nltk.stem.porter import PorterStemmer#stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using sqlite to read data from data base:\n",
    "con=sqlite3.connect('./amazon-fine-food-reviews/database.sqlite')\n",
    "#filtering positive and negative data\n",
    "filtered_data=pd.read_sql_query(\"\"\"\n",
    "SELECT *\n",
    "FROM REVIEWS\n",
    "WHERE SCORE !=3\n",
    "\"\"\",con)\n",
    "def partition(x):\n",
    "    if x<3:\n",
    "        return 'negative'\n",
    "    return 'positive'\n",
    "actualscore=filtered_data['Score']\n",
    "positive_negative=actualscore.map(partition)\n",
    "filtered_data['Score']=positive_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(525814, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>ADT0SRK1MGOEU</td>\n",
       "      <td>Twoapennything</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1342051200</td>\n",
       "      <td>Nice Taffy</td>\n",
       "      <td>I got a wild hair for taffy and ordered this f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1SP2KVKFXXRU1</td>\n",
       "      <td>David C. Sullivan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1340150400</td>\n",
       "      <td>Great!  Just as good as the expensive brands!</td>\n",
       "      <td>This saltwater taffy had great flavors and was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A3JRGQVEQN31IQ</td>\n",
       "      <td>Pamela G. Williams</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1336003200</td>\n",
       "      <td>Wonderful, tasty taffy</td>\n",
       "      <td>This taffy is so good.  It is very soft and ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>B000E7L2R4</td>\n",
       "      <td>A1MZYO9TZK0BBI</td>\n",
       "      <td>R. James</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1322006400</td>\n",
       "      <td>Yay Barley</td>\n",
       "      <td>Right now I'm mostly just sprouting this so my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>B00171APVA</td>\n",
       "      <td>A21BT40VZCCYT4</td>\n",
       "      <td>Carol A. Reed</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1351209600</td>\n",
       "      <td>Healthy Dog Food</td>\n",
       "      <td>This is a very healthy dog food. Good for thei...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "5   6  B006K2ZZ7K   ADT0SRK1MGOEU                   Twoapennything   \n",
       "6   7  B006K2ZZ7K  A1SP2KVKFXXRU1                David C. Sullivan   \n",
       "7   8  B006K2ZZ7K  A3JRGQVEQN31IQ               Pamela G. Williams   \n",
       "8   9  B000E7L2R4  A1MZYO9TZK0BBI                         R. James   \n",
       "9  10  B00171APVA  A21BT40VZCCYT4                    Carol A. Reed   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator     Score        Time  \\\n",
       "0                     1                       1  positive  1303862400   \n",
       "1                     0                       0  negative  1346976000   \n",
       "2                     1                       1  positive  1219017600   \n",
       "3                     3                       3  negative  1307923200   \n",
       "4                     0                       0  positive  1350777600   \n",
       "5                     0                       0  positive  1342051200   \n",
       "6                     0                       0  positive  1340150400   \n",
       "7                     0                       0  positive  1336003200   \n",
       "8                     1                       1  positive  1322006400   \n",
       "9                     0                       0  positive  1351209600   \n",
       "\n",
       "                                         Summary  \\\n",
       "0                          Good Quality Dog Food   \n",
       "1                              Not as Advertised   \n",
       "2                          \"Delight\" says it all   \n",
       "3                                 Cough Medicine   \n",
       "4                                    Great taffy   \n",
       "5                                     Nice Taffy   \n",
       "6  Great!  Just as good as the expensive brands!   \n",
       "7                         Wonderful, tasty taffy   \n",
       "8                                     Yay Barley   \n",
       "9                               Healthy Dog Food   \n",
       "\n",
       "                                                Text  \n",
       "0  I have bought several of the Vitality canned d...  \n",
       "1  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  This is a confection that has been around a fe...  \n",
       "3  If you are looking for the secret ingredient i...  \n",
       "4  Great taffy at a great price.  There was a wid...  \n",
       "5  I got a wild hair for taffy and ordered this f...  \n",
       "6  This saltwater taffy had great flavors and was...  \n",
       "7  This taffy is so good.  It is very soft and ch...  \n",
       "8  Right now I'm mostly just sprouting this so my...  \n",
       "9  This is a very healthy dog food. Good for thei...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(filtered_data.shape)\n",
    "filtered_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing duplicate data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>78445</td>\n",
       "      <td>B000HDL1RQ</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>138317</td>\n",
       "      <td>B000HDOPYC</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138277</td>\n",
       "      <td>B000HDOPYM</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73791</td>\n",
       "      <td>B000HDOPZG</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>155049</td>\n",
       "      <td>B000PAQ75C</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id   ProductId         UserId      ProfileName  HelpfulnessNumerator  \\\n",
       "0   78445  B000HDL1RQ  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "1  138317  B000HDOPYC  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "2  138277  B000HDOPYM  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "3   73791  B000HDOPZG  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "4  155049  B000PAQ75C  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "\n",
       "   HelpfulnessDenominator  Score        Time  \\\n",
       "0                       2      5  1199577600   \n",
       "1                       2      5  1199577600   \n",
       "2                       2      5  1199577600   \n",
       "3                       2      5  1199577600   \n",
       "4                       2      5  1199577600   \n",
       "\n",
       "                             Summary  \\\n",
       "0  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "1  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "2  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "3  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "4  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "\n",
       "                                                Text  \n",
       "0  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "1  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "2  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "3  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "4  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display=pd.read_sql_query(\"\"\"\n",
    "SELECT *\n",
    "FROM REVIEWS\n",
    "WHERE SCORE !=3 AND USERID=\"AR5J8UI46CURR\"\n",
    "ORDER BY PRODUCTID\n",
    "\"\"\",con)\n",
    "display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sort by the values along either axis.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "        by : str or list of str\n",
      "            Name or list of names to sort by.\n",
      "\n",
      "            - if `axis` is 0 or `'index'` then `by` may contain index\n",
      "              levels and/or column labels.\n",
      "            - if `axis` is 1 or `'columns'` then `by` may contain column\n",
      "              levels and/or index labels.\n",
      "\n",
      "            .. versionchanged:: 0.23.0\n",
      "\n",
      "               Allow specifying index or column level names.\n",
      "axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      "     Axis to be sorted.\n",
      "ascending : bool or list of bool, default True\n",
      "     Sort ascending vs. descending. Specify list for multiple sort\n",
      "     orders.  If this is a list of bools, must match the length of\n",
      "     the by.\n",
      "inplace : bool, default False\n",
      "     If True, perform operation in-place.\n",
      "kind : {'quicksort', 'mergesort', 'heapsort'}, default 'quicksort'\n",
      "     Choice of sorting algorithm. See also ndarray.np.sort for more\n",
      "     information.  `mergesort` is the only stable algorithm. For\n",
      "     DataFrames, this option is only applied when sorting on a single\n",
      "     column or label.\n",
      "na_position : {'first', 'last'}, default 'last'\n",
      "     Puts NaNs at the beginning if `first`; `last` puts NaNs at the\n",
      "     end.\n",
      "ignore_index : bool, default False\n",
      "     If True, the resulting axis will be labeled 0, 1, …, n - 1.\n",
      "\n",
      "     .. versionadded:: 1.0.0\n",
      "\n",
      "key : callable, optional\n",
      "    Apply the key function to the values\n",
      "    before sorting. This is similar to the `key` argument in the\n",
      "    builtin :meth:`sorted` function, with the notable difference that\n",
      "    this `key` function should be *vectorized*. It should expect a\n",
      "    ``Series`` and return a Series with the same shape as the input.\n",
      "    It will be applied to each column in `by` independently.\n",
      "\n",
      "    .. versionadded:: 1.1.0\n",
      "\n",
      "Returns\n",
      "-------\n",
      "DataFrame or None\n",
      "    DataFrame with sorted values if inplace=False, None otherwise.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "DataFrame.sort_index : Sort a DataFrame by the index.\n",
      "Series.sort_values : Similar method for a Series.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> df = pd.DataFrame({\n",
      "...     'col1': ['A', 'A', 'B', np.nan, 'D', 'C'],\n",
      "...     'col2': [2, 1, 9, 8, 7, 4],\n",
      "...     'col3': [0, 1, 9, 4, 2, 3],\n",
      "...     'col4': ['a', 'B', 'c', 'D', 'e', 'F']\n",
      "... })\n",
      ">>> df\n",
      "  col1  col2  col3 col4\n",
      "0    A     2     0    a\n",
      "1    A     1     1    B\n",
      "2    B     9     9    c\n",
      "3  NaN     8     4    D\n",
      "4    D     7     2    e\n",
      "5    C     4     3    F\n",
      "\n",
      "Sort by col1\n",
      "\n",
      ">>> df.sort_values(by=['col1'])\n",
      "  col1  col2  col3 col4\n",
      "0    A     2     0    a\n",
      "1    A     1     1    B\n",
      "2    B     9     9    c\n",
      "5    C     4     3    F\n",
      "4    D     7     2    e\n",
      "3  NaN     8     4    D\n",
      "\n",
      "Sort by multiple columns\n",
      "\n",
      ">>> df.sort_values(by=['col1', 'col2'])\n",
      "  col1  col2  col3 col4\n",
      "1    A     1     1    B\n",
      "0    A     2     0    a\n",
      "2    B     9     9    c\n",
      "5    C     4     3    F\n",
      "4    D     7     2    e\n",
      "3  NaN     8     4    D\n",
      "\n",
      "Sort Descending\n",
      "\n",
      ">>> df.sort_values(by='col1', ascending=False)\n",
      "  col1  col2  col3 col4\n",
      "4    D     7     2    e\n",
      "5    C     4     3    F\n",
      "2    B     9     9    c\n",
      "0    A     2     0    a\n",
      "1    A     1     1    B\n",
      "3  NaN     8     4    D\n",
      "\n",
      "Putting NAs first\n",
      "\n",
      ">>> df.sort_values(by='col1', ascending=False, na_position='first')\n",
      "  col1  col2  col3 col4\n",
      "3  NaN     8     4    D\n",
      "4    D     7     2    e\n",
      "5    C     4     3    F\n",
      "2    B     9     9    c\n",
      "0    A     2     0    a\n",
      "1    A     1     1    B\n",
      "\n",
      "Sorting with a key function\n",
      "\n",
      ">>> df.sort_values(by='col4', key=lambda col: col.str.lower())\n",
      "   col1  col2  col3 col4\n",
      "0    A     2     0    a\n",
      "1    A     1     1    B\n",
      "2    B     9     9    c\n",
      "3  NaN     8     4    D\n",
      "4    D     7     2    e\n",
      "5    C     4     3    F\n",
      "\n",
      "********************************\n",
      "\n",
      "        Return DataFrame with duplicate rows removed.\n",
      "\n",
      "        Considering certain columns is optional. Indexes, including time indexes\n",
      "        are ignored.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        subset : column label or sequence of labels, optional\n",
      "            Only consider certain columns for identifying duplicates, by\n",
      "            default use all of the columns.\n",
      "        keep : {'first', 'last', False}, default 'first'\n",
      "            Determines which duplicates (if any) to keep.\n",
      "            - ``first`` : Drop duplicates except for the first occurrence.\n",
      "            - ``last`` : Drop duplicates except for the last occurrence.\n",
      "            - False : Drop all duplicates.\n",
      "        inplace : bool, default False\n",
      "            Whether to drop duplicates in place or to return a copy.\n",
      "        ignore_index : bool, default False\n",
      "            If True, the resulting axis will be labeled 0, 1, …, n - 1.\n",
      "\n",
      "            .. versionadded:: 1.0.0\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        DataFrame\n",
      "            DataFrame with duplicates removed or None if ``inplace=True``.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        DataFrame.value_counts: Count unique combinations of columns.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        Consider dataset containing ramen rating.\n",
      "\n",
      "        >>> df = pd.DataFrame({\n",
      "        ...     'brand': ['Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'],\n",
      "        ...     'style': ['cup', 'cup', 'cup', 'pack', 'pack'],\n",
      "        ...     'rating': [4, 4, 3.5, 15, 5]\n",
      "        ... })\n",
      "        >>> df\n",
      "            brand style  rating\n",
      "        0  Yum Yum   cup     4.0\n",
      "        1  Yum Yum   cup     4.0\n",
      "        2  Indomie   cup     3.5\n",
      "        3  Indomie  pack    15.0\n",
      "        4  Indomie  pack     5.0\n",
      "\n",
      "        By default, it removes duplicate rows based on all columns.\n",
      "\n",
      "        >>> df.drop_duplicates()\n",
      "            brand style  rating\n",
      "        0  Yum Yum   cup     4.0\n",
      "        2  Indomie   cup     3.5\n",
      "        3  Indomie  pack    15.0\n",
      "        4  Indomie  pack     5.0\n",
      "\n",
      "        To remove duplicates on specific column(s), use ``subset``.\n",
      "\n",
      "        >>> df.drop_duplicates(subset=['brand'])\n",
      "            brand style  rating\n",
      "        0  Yum Yum   cup     4.0\n",
      "        2  Indomie   cup     3.5\n",
      "\n",
      "        To remove duplicates and keep last occurences, use ``keep``.\n",
      "\n",
      "        >>> df.drop_duplicates(subset=['brand', 'style'], keep='last')\n",
      "            brand style  rating\n",
      "        1  Yum Yum   cup     4.0\n",
      "        2  Indomie   cup     3.5\n",
      "        4  Indomie  pack     5.0\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(filtered_data.sort_values.__doc__)\n",
    "print('********************************')\n",
    "print(filtered_data.drop_duplicates.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364173, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sorting data\n",
    "#inplace means to return the output after proforming task\n",
    "sorted_data=filtered_data.sort_values('ProductId',axis=0,ascending=True)\n",
    "#removing duplicate\n",
    "final=filtered_data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"},keep='first',inplace=False)\n",
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final=final[final.HelpfulnessNumerator<=final.HelpfulnessDenominator]\n",
    "final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect=CountVectorizer()\n",
    "final_count=count_vect.fit_transform(final[\"Text\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert a collection of text documents to a matrix of token counts\n",
      "\n",
      "    This implementation produces a sparse representation of the counts using\n",
      "    scipy.sparse.csr_matrix.\n",
      "\n",
      "    If you do not provide an a-priori dictionary and you do not use an analyzer\n",
      "    that does some kind of feature selection then the number of features will\n",
      "    be equal to the vocabulary size found by analyzing the data.\n",
      "\n",
      "    Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    input : string {'filename', 'file', 'content'}, default='content'\n",
      "        If 'filename', the sequence passed as an argument to fit is\n",
      "        expected to be a list of filenames that need reading to fetch\n",
      "        the raw content to analyze.\n",
      "\n",
      "        If 'file', the sequence items must have a 'read' method (file-like\n",
      "        object) that is called to fetch the bytes in memory.\n",
      "\n",
      "        Otherwise the input is expected to be a sequence of items that\n",
      "        can be of type string or byte.\n",
      "\n",
      "    encoding : string, default='utf-8'\n",
      "        If bytes or files are given to analyze, this encoding is used to\n",
      "        decode.\n",
      "\n",
      "    decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
      "        Instruction on what to do if a byte sequence is given to analyze that\n",
      "        contains characters not of the given `encoding`. By default, it is\n",
      "        'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
      "        values are 'ignore' and 'replace'.\n",
      "\n",
      "    strip_accents : {'ascii', 'unicode'}, default=None\n",
      "        Remove accents and perform other character normalization\n",
      "        during the preprocessing step.\n",
      "        'ascii' is a fast method that only works on characters that have\n",
      "        an direct ASCII mapping.\n",
      "        'unicode' is a slightly slower method that works on any characters.\n",
      "        None (default) does nothing.\n",
      "\n",
      "        Both 'ascii' and 'unicode' use NFKD normalization from\n",
      "        :func:`unicodedata.normalize`.\n",
      "\n",
      "    lowercase : bool, default=True\n",
      "        Convert all characters to lowercase before tokenizing.\n",
      "\n",
      "    preprocessor : callable, default=None\n",
      "        Override the preprocessing (string transformation) stage while\n",
      "        preserving the tokenizing and n-grams generation steps.\n",
      "        Only applies if ``analyzer is not callable``.\n",
      "\n",
      "    tokenizer : callable, default=None\n",
      "        Override the string tokenization step while preserving the\n",
      "        preprocessing and n-grams generation steps.\n",
      "        Only applies if ``analyzer == 'word'``.\n",
      "\n",
      "    stop_words : string {'english'}, list, default=None\n",
      "        If 'english', a built-in stop word list for English is used.\n",
      "        There are several known issues with 'english' and you should\n",
      "        consider an alternative (see :ref:`stop_words`).\n",
      "\n",
      "        If a list, that list is assumed to contain stop words, all of which\n",
      "        will be removed from the resulting tokens.\n",
      "        Only applies if ``analyzer == 'word'``.\n",
      "\n",
      "        If None, no stop words will be used. max_df can be set to a value\n",
      "        in the range [0.7, 1.0) to automatically detect and filter stop\n",
      "        words based on intra corpus document frequency of terms.\n",
      "\n",
      "    token_pattern : string\n",
      "        Regular expression denoting what constitutes a \"token\", only used\n",
      "        if ``analyzer == 'word'``. The default regexp select tokens of 2\n",
      "        or more alphanumeric characters (punctuation is completely ignored\n",
      "        and always treated as a token separator).\n",
      "\n",
      "    ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
      "        The lower and upper boundary of the range of n-values for different\n",
      "        word n-grams or char n-grams to be extracted. All values of n such\n",
      "        such that min_n <= n <= max_n will be used. For example an\n",
      "        ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n",
      "        unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n",
      "        Only applies if ``analyzer is not callable``.\n",
      "\n",
      "    analyzer : string, {'word', 'char', 'char_wb'} or callable,             default='word'\n",
      "        Whether the feature should be made of word n-gram or character\n",
      "        n-grams.\n",
      "        Option 'char_wb' creates character n-grams only from text inside\n",
      "        word boundaries; n-grams at the edges of words are padded with space.\n",
      "\n",
      "        If a callable is passed it is used to extract the sequence of features\n",
      "        out of the raw, unprocessed input.\n",
      "\n",
      "        .. versionchanged:: 0.21\n",
      "\n",
      "        Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
      "        first read from the file and then passed to the given callable\n",
      "        analyzer.\n",
      "\n",
      "    max_df : float in range [0.0, 1.0] or int, default=1.0\n",
      "        When building the vocabulary ignore terms that have a document\n",
      "        frequency strictly higher than the given threshold (corpus-specific\n",
      "        stop words).\n",
      "        If float, the parameter represents a proportion of documents, integer\n",
      "        absolute counts.\n",
      "        This parameter is ignored if vocabulary is not None.\n",
      "\n",
      "    min_df : float in range [0.0, 1.0] or int, default=1\n",
      "        When building the vocabulary ignore terms that have a document\n",
      "        frequency strictly lower than the given threshold. This value is also\n",
      "        called cut-off in the literature.\n",
      "        If float, the parameter represents a proportion of documents, integer\n",
      "        absolute counts.\n",
      "        This parameter is ignored if vocabulary is not None.\n",
      "\n",
      "    max_features : int, default=None\n",
      "        If not None, build a vocabulary that only consider the top\n",
      "        max_features ordered by term frequency across the corpus.\n",
      "\n",
      "        This parameter is ignored if vocabulary is not None.\n",
      "\n",
      "    vocabulary : Mapping or iterable, default=None\n",
      "        Either a Mapping (e.g., a dict) where keys are terms and values are\n",
      "        indices in the feature matrix, or an iterable over terms. If not\n",
      "        given, a vocabulary is determined from the input documents. Indices\n",
      "        in the mapping should not be repeated and should not have any gap\n",
      "        between 0 and the largest index.\n",
      "\n",
      "    binary : bool, default=False\n",
      "        If True, all non zero counts are set to 1. This is useful for discrete\n",
      "        probabilistic models that model binary events rather than integer\n",
      "        counts.\n",
      "\n",
      "    dtype : type, default=np.int64\n",
      "        Type of the matrix returned by fit_transform() or transform().\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    vocabulary_ : dict\n",
      "        A mapping of terms to feature indices.\n",
      "\n",
      "    fixed_vocabulary_: boolean\n",
      "        True if a fixed vocabulary of term to indices mapping\n",
      "        is provided by the user\n",
      "\n",
      "    stop_words_ : set\n",
      "        Terms that were ignored because they either:\n",
      "\n",
      "          - occurred in too many documents (`max_df`)\n",
      "          - occurred in too few documents (`min_df`)\n",
      "          - were cut off by feature selection (`max_features`).\n",
      "\n",
      "        This is only available if no vocabulary was given.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.feature_extraction.text import CountVectorizer\n",
      "    >>> corpus = [\n",
      "    ...     'This is the first document.',\n",
      "    ...     'This document is the second document.',\n",
      "    ...     'And this is the third one.',\n",
      "    ...     'Is this the first document?',\n",
      "    ... ]\n",
      "    >>> vectorizer = CountVectorizer()\n",
      "    >>> X = vectorizer.fit_transform(corpus)\n",
      "    >>> print(vectorizer.get_feature_names())\n",
      "    ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "    >>> print(X.toarray())\n",
      "    [[0 1 1 1 0 0 1 0 1]\n",
      "     [0 2 0 1 0 1 1 0 1]\n",
      "     [1 0 0 1 1 0 1 1 1]\n",
      "     [0 1 1 1 0 0 1 0 1]]\n",
      "    >>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
      "    >>> X2 = vectorizer2.fit_transform(corpus)\n",
      "    >>> print(vectorizer2.get_feature_names())\n",
      "    ['and this', 'document is', 'first document', 'is the', 'is this',\n",
      "    'second document', 'the first', 'the second', 'the third', 'third one',\n",
      "     'this document', 'this is', 'this the']\n",
      "     >>> print(X2.toarray())\n",
      "     [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
      "     [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
      "     [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
      "     [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    HashingVectorizer, TfidfVectorizer\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    The ``stop_words_`` attribute can get large and increase the model size\n",
      "    when pickling. This attribute is provided only for introspection and can\n",
      "    be safely removed using delattr or set to None before pickling.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(count_vect.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(final_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre processing of text:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "I don't know if it's the cactus or the tequila or just the unique combination of ingredients, but the flavour of this hot sauce makes it one of a kind!  We picked up a bottle once on a trip we were on and brought it back home with us and were totally blown away!  When we realized that we simply couldn't find it anywhere in our city we were bummed.<br /><br />Now, because of the magic of the internet, we have a case of the sauce and are ecstatic because of it.<br /><br />If you love hot sauce..I mean really love hot sauce, but don't want a sauce that tastelessly burns your throat, grab a bottle of Tequila Picante Gourmet de Inclan.  Just realize that once you taste it, you will never want to use any other sauce.<br /><br />Thank you for the personal, incredible service!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer# break the sentence into word\n",
    "#finding all html tag in the dat given\n",
    "i=0\n",
    "for sent in final['Text'].values:\n",
    "    if(len(re.findall('<.*?>',sent))):\n",
    "        print(i)\n",
    "        print(sent)\n",
    "        break;\n",
    "    i+=1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Snowball Stemmer\n",
      "\n",
      "    The following languages are supported:\n",
      "    Arabic, Danish, Dutch, English, Finnish, French, German,\n",
      "    Hungarian, Italian, Norwegian, Portuguese, Romanian, Russian,\n",
      "    Spanish and Swedish.\n",
      "\n",
      "    The algorithm for English is documented here:\n",
      "\n",
      "        Porter, M. \"An algorithm for suffix stripping.\"\n",
      "        Program 14.3 (1980): 130-137.\n",
      "\n",
      "    The algorithms have been developed by Martin Porter.\n",
      "    These stemmers are called Snowball, because Porter created\n",
      "    a programming language with this name for creating\n",
      "    new stemming algorithms. There is more information available\n",
      "    at http://snowball.tartarus.org/\n",
      "\n",
      "    The stemmer is invoked as shown below:\n",
      "\n",
      "    >>> from nltk.stem import SnowballStemmer\n",
      "    >>> print(\" \".join(SnowballStemmer.languages)) # See which languages are supported\n",
      "    arabic danish dutch english finnish french german hungarian\n",
      "    italian norwegian porter portuguese romanian russian\n",
      "    spanish swedish\n",
      "    >>> stemmer = SnowballStemmer(\"german\") # Choose a language\n",
      "    >>> stemmer.stem(\"Autobahnen\") # Stem a word\n",
      "    'autobahn'\n",
      "\n",
      "    Invoking the stemmers that way is useful if you do not know the\n",
      "    language to be stemmed at runtime. Alternatively, if you already know\n",
      "    the language, then you can invoke the language specific stemmer directly:\n",
      "\n",
      "    >>> from nltk.stem.snowball import GermanStemmer\n",
      "    >>> stemmer = GermanStemmer()\n",
      "    >>> stemmer.stem(\"Autobahnen\")\n",
      "    'autobahn'\n",
      "\n",
      "    :param language: The language whose subclass is instantiated.\n",
      "    :type language: str or unicode\n",
      "    :param ignore_stopwords: If set to True, stopwords are\n",
      "                             not stemmed and returned unchanged.\n",
      "                             Set to False by default.\n",
      "    :type ignore_stopwords: bool\n",
      "    :raise ValueError: If there is no stemmer for the specified\n",
      "                           language, a ValueError is raised.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(nltk.stem.snowball.SnowballStemmer.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/mahaseth_rahul/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop=set(stopwords.words('english'))\n",
    "#initialising the snow ball stemmer\n",
    "sno=nltk.stem.snowball.SnowballStemmer('english')#derive root word \n",
    "def cleanhtml(sentence):\n",
    "    cleanr=re.compile('<.*?>')\n",
    "    cleantext=re.sub(cleanr,' ',sentence)\n",
    "    return cleantext\n",
    "def cleanpunc(sentence):\n",
    "    cleaned =re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned=re.sub(r'[.|,|)|(|\\|/]',r'',sentence)\n",
    "    return cleaned\n",
    "stop   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for implenting step-by-step the checks mentioned in pre-processing\n",
    "from tqdm import tqdm\n",
    "i=0\n",
    "str1=' '\n",
    "final_string=[]\n",
    "all_positive_words=[]\n",
    "all_negative_words=[]\n",
    "s=''\n",
    "for sent in tqdm(final['Text'].values):\n",
    "    filtered_setentence=[]\n",
    "    sent=cleanhtml(sent)\n",
    "    for w in sent.split():\n",
    "        #print(sent.split())\n",
    "        for cleaned_word in cleanpunc(w).split():\n",
    "            #print(cleanpunc(w).split())\n",
    "            if ((cleaned_word.isalpha())&(len(cleaned_word)>2)):              \n",
    "                if(cleaned_word.lower() not in stop):\n",
    "                    s=(sno.stem(cleaned_word.lower())).encode('utf8')\n",
    "                    filtered_sentence.append(s)\n",
    "                    if(final['Score'].values)[i]=='positive':\n",
    "                        all_positive_words.append(s)\n",
    "                    if(final[\"Score\"].values)[i]=='negative':\n",
    "                        all_negative_words.append(s)\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "    str1=b\" \".join(filtered_sentence)\n",
    "    final_string.append(str1)\n",
    "    i+=1\n",
    "                 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/47091490/4084039\n",
    "import re\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gist.github.com/sebleier/554280\n",
    "# we are removing the words from the stop words list: 'no', 'nor', 'not'\n",
    "# <br /><br /> ==> after the above steps, we are getting \"br br\"\n",
    "# we are including them into stop words list\n",
    "# instead of <br /> if we have <br/> these tags would have revmoved in the 1st step\n",
    "\n",
    "stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 364171/364171 [02:17<00:00, 2657.28it/s]\n"
     ]
    }
   ],
   "source": [
    "#print(final[\"Text\"].values)\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup   \n",
    "preprocessed_reviewed_text=[]\n",
    "for sentence in tqdm(final[\"Text\"].values):\n",
    "    sentence=re.sub(r\"http\\S+\",\"\",sentence)#remove all the url from the rex\n",
    "    sentence=BeautifulSoup(sentence,\"lxml\").get_text()\n",
    "    sentence=decontracted(sentence)\n",
    "    sentence=re.sub(\"\\S*\\d\\S*\",\"\",sentence).strip()\n",
    "    sentence=re.sub('[^A-Za-z]+',\" \",sentence)\n",
    "    #print(sentence)\n",
    "    sentence = ' '.join(e.lower() for e in sentence.split() if e.lower() not in stopwords)\n",
    "    #print(sentence)\n",
    "    preprocessed_reviewed_text.append(sentence.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better',\n",
       " 'product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted not sure error vendor intended represent product jumbo',\n",
       " 'confection around centuries light pillowy citrus gelatin nuts case filberts cut tiny squares liberally coated powdered sugar tiny mouthful heaven not chewy flavorful highly recommend yummy treat familiar story c lewis lion witch wardrobe treat seduces edmund selling brother sisters witch',\n",
       " 'looking secret ingredient robitussin believe found got addition root beer extract ordered good made cherry soda flavor medicinal',\n",
       " 'great taffy great price wide assortment yummy taffy delivery quick taffy lover deal',\n",
       " 'got wild hair taffy ordered five pound bag taffy enjoyable many flavors watermelon root beer melon peppermint grape etc complaint bit much red black licorice flavored pieces not particular favorites kids husband lasted two weeks would recommend brand taffy delightful treat',\n",
       " 'saltwater taffy great flavors soft chewy candy individually wrapped well none candies stuck together happen expensive version fralinger would highly recommend candy served beach themed party everyone loved',\n",
       " 'taffy good soft chewy flavors amazing would definitely recommend buying satisfying']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_reviewed_text[0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['CleanedText']=preprocessed_reviewed_text\n",
    "#adding a column of filter dat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>CleanedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>bought several vitality canned dog food produc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>product arrived labeled jumbo salted peanuts p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>confection around centuries light pillowy citr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>looking secret ingredient robitussin believe f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>great taffy great price wide assortment yummy ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator     Score        Time  \\\n",
       "0                     1                       1  positive  1303862400   \n",
       "1                     0                       0  negative  1346976000   \n",
       "2                     1                       1  positive  1219017600   \n",
       "3                     3                       3  negative  1307923200   \n",
       "4                     0                       0  positive  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \\\n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...   \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...   \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...   \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...   \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...   \n",
       "\n",
       "                                         CleanedText  \n",
       "0  bought several vitality canned dog food produc...  \n",
       "1  product arrived labeled jumbo salted peanuts p...  \n",
       "2  confection around centuries light pillowy citr...  \n",
       "3  looking secret ingredient robitussin believe f...  \n",
       "4  great taffy great price wide assortment yummy ...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQLite database connection object.\n",
      "SQLite database cursor class.\n",
      "\n",
      "        Write records stored in a DataFrame to a SQL database.\n",
      "\n",
      "        Databases supported by SQLAlchemy [1]_ are supported. Tables can be\n",
      "        newly created, appended to, or overwritten.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        name : str\n",
      "            Name of SQL table.\n",
      "        con : sqlalchemy.engine.(Engine or Connection) or sqlite3.Connection\n",
      "            Using SQLAlchemy makes it possible to use any DB supported by that\n",
      "            library. Legacy support is provided for sqlite3.Connection objects. The user\n",
      "            is responsible for engine disposal and connection closure for the SQLAlchemy\n",
      "            connectable See `here                 <https://docs.sqlalchemy.org/en/13/core/connections.html>`_.\n",
      "\n",
      "        schema : str, optional\n",
      "            Specify the schema (if database flavor supports this). If None, use\n",
      "            default schema.\n",
      "        if_exists : {'fail', 'replace', 'append'}, default 'fail'\n",
      "            How to behave if the table already exists.\n",
      "\n",
      "            * fail: Raise a ValueError.\n",
      "            * replace: Drop the table before inserting new values.\n",
      "            * append: Insert new values to the existing table.\n",
      "\n",
      "        index : bool, default True\n",
      "            Write DataFrame index as a column. Uses `index_label` as the column\n",
      "            name in the table.\n",
      "        index_label : str or sequence, default None\n",
      "            Column label for index column(s). If None is given (default) and\n",
      "            `index` is True, then the index names are used.\n",
      "            A sequence should be given if the DataFrame uses MultiIndex.\n",
      "        chunksize : int, optional\n",
      "            Specify the number of rows in each batch to be written at a time.\n",
      "            By default, all rows will be written at once.\n",
      "        dtype : dict or scalar, optional\n",
      "            Specifying the datatype for columns. If a dictionary is used, the\n",
      "            keys should be the column names and the values should be the\n",
      "            SQLAlchemy types or strings for the sqlite3 legacy mode. If a\n",
      "            scalar is provided, it will be applied to all columns.\n",
      "        method : {None, 'multi', callable}, optional\n",
      "            Controls the SQL insertion clause used:\n",
      "\n",
      "            * None : Uses standard SQL ``INSERT`` clause (one per row).\n",
      "            * 'multi': Pass multiple values in a single ``INSERT`` clause.\n",
      "            * callable with signature ``(pd_table, conn, keys, data_iter)``.\n",
      "\n",
      "            Details and a sample callable implementation can be found in the\n",
      "            section :ref:`insert method <io.sql.method>`.\n",
      "\n",
      "            .. versionadded:: 0.24.0\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        ValueError\n",
      "            When the table already exists and `if_exists` is 'fail' (the\n",
      "            default).\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        read_sql : Read a DataFrame from a table.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        Timezone aware datetime columns will be written as\n",
      "        ``Timestamp with timezone`` type with SQLAlchemy if supported by the\n",
      "        database. Otherwise, the datetimes will be stored as timezone unaware\n",
      "        timestamps local to the original timezone.\n",
      "\n",
      "        .. versionadded:: 0.24.0\n",
      "\n",
      "        References\n",
      "        ----------\n",
      "        .. [1] https://docs.sqlalchemy.org\n",
      "        .. [2] https://www.python.org/dev/peps/pep-0249/\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        Create an in-memory SQLite database.\n",
      "\n",
      "        >>> from sqlalchemy import create_engine\n",
      "        >>> engine = create_engine('sqlite://', echo=False)\n",
      "\n",
      "        Create a table from scratch with 3 rows.\n",
      "\n",
      "        >>> df = pd.DataFrame({'name' : ['User 1', 'User 2', 'User 3']})\n",
      "        >>> df\n",
      "             name\n",
      "        0  User 1\n",
      "        1  User 2\n",
      "        2  User 3\n",
      "\n",
      "        >>> df.to_sql('users', con=engine)\n",
      "        >>> engine.execute(\"SELECT * FROM users\").fetchall()\n",
      "        [(0, 'User 1'), (1, 'User 2'), (2, 'User 3')]\n",
      "\n",
      "        An `sqlalchemy.engine.Connection` can also be passed to to `con`:\n",
      "        >>> with engine.begin() as connection:\n",
      "        ...     df1 = pd.DataFrame({'name' : ['User 4', 'User 5']})\n",
      "        ...     df1.to_sql('users', con=connection, if_exists='append')\n",
      "\n",
      "        This is allowed to support operations that require that the same\n",
      "        DBAPI connection is used for the entire operation.\n",
      "\n",
      "        >>> df2 = pd.DataFrame({'name' : ['User 6', 'User 7']})\n",
      "        >>> df2.to_sql('users', con=engine, if_exists='append')\n",
      "        >>> engine.execute(\"SELECT * FROM users\").fetchall()\n",
      "        [(0, 'User 1'), (1, 'User 2'), (2, 'User 3'),\n",
      "         (0, 'User 4'), (1, 'User 5'), (0, 'User 6'),\n",
      "         (1, 'User 7')]\n",
      "\n",
      "        Overwrite the table with just ``df2``.\n",
      "\n",
      "        >>> df2.to_sql('users', con=engine, if_exists='replace',\n",
      "        ...            index_label='id')\n",
      "        >>> engine.execute(\"SELECT * FROM users\").fetchall()\n",
      "        [(0, 'User 6'), (1, 'User 7')]\n",
      "\n",
      "        Specify the dtype (especially useful for integers with missing values).\n",
      "        Notice that while pandas is forced to store the data as floating point,\n",
      "        the database supports nullable integers. When fetching the data with\n",
      "        Python, we get back integer scalars.\n",
      "\n",
      "        >>> df = pd.DataFrame({\"A\": [1, None, 2]})\n",
      "        >>> df\n",
      "             A\n",
      "        0  1.0\n",
      "        1  NaN\n",
      "        2  2.0\n",
      "\n",
      "        >>> from sqlalchemy.types import Integer\n",
      "        >>> df.to_sql('integers', con=engine, index=False,\n",
      "        ...           dtype={\"A\": Integer()})\n",
      "\n",
      "        >>> engine.execute(\"SELECT * FROM integers\").fetchall()\n",
      "        [(1,), (None,), (2,)]\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "conn=sqlite3.connect('final.sqlite')\n",
    "print(conn.__doc__)\n",
    "c=conn.cursor()\n",
    "print(c.__doc__)\n",
    "conn.text_factory= str\n",
    "final.to_sql('Reviews',conn,schema=None, if_exists='replace')\n",
    "print(final.to_sql.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-grams and n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AbstractLazySequence',\n",
       " 'AffixTagger',\n",
       " 'AlignedSent',\n",
       " 'Alignment',\n",
       " 'AnnotationTask',\n",
       " 'ApplicationExpression',\n",
       " 'Assignment',\n",
       " 'BigramAssocMeasures',\n",
       " 'BigramCollocationFinder',\n",
       " 'BigramTagger',\n",
       " 'BinaryMaxentFeatureEncoding',\n",
       " 'BlanklineTokenizer',\n",
       " 'BllipParser',\n",
       " 'BottomUpChartParser',\n",
       " 'BottomUpLeftCornerChartParser',\n",
       " 'BottomUpProbabilisticChartParser',\n",
       " 'Boxer',\n",
       " 'BrillTagger',\n",
       " 'BrillTaggerTrainer',\n",
       " 'CFG',\n",
       " 'CRFTagger',\n",
       " 'CfgReadingCommand',\n",
       " 'ChartParser',\n",
       " 'ChunkParserI',\n",
       " 'ChunkScore',\n",
       " 'Cistem',\n",
       " 'ClassifierBasedPOSTagger',\n",
       " 'ClassifierBasedTagger',\n",
       " 'ClassifierI',\n",
       " 'ConcordanceIndex',\n",
       " 'ConditionalExponentialClassifier',\n",
       " 'ConditionalFreqDist',\n",
       " 'ConditionalProbDist',\n",
       " 'ConditionalProbDistI',\n",
       " 'ConfusionMatrix',\n",
       " 'ContextIndex',\n",
       " 'ContextTagger',\n",
       " 'ContingencyMeasures',\n",
       " 'CoreNLPDependencyParser',\n",
       " 'CoreNLPParser',\n",
       " 'Counter',\n",
       " 'CrossValidationProbDist',\n",
       " 'DRS',\n",
       " 'DecisionTreeClassifier',\n",
       " 'DefaultTagger',\n",
       " 'DependencyEvaluator',\n",
       " 'DependencyGrammar',\n",
       " 'DependencyGraph',\n",
       " 'DependencyProduction',\n",
       " 'DictionaryConditionalProbDist',\n",
       " 'DictionaryProbDist',\n",
       " 'DiscourseTester',\n",
       " 'DrtExpression',\n",
       " 'DrtGlueReadingCommand',\n",
       " 'ELEProbDist',\n",
       " 'EarleyChartParser',\n",
       " 'Expression',\n",
       " 'FStructure',\n",
       " 'FeatDict',\n",
       " 'FeatList',\n",
       " 'FeatStruct',\n",
       " 'FeatStructReader',\n",
       " 'Feature',\n",
       " 'FeatureBottomUpChartParser',\n",
       " 'FeatureBottomUpLeftCornerChartParser',\n",
       " 'FeatureChartParser',\n",
       " 'FeatureEarleyChartParser',\n",
       " 'FeatureIncrementalBottomUpChartParser',\n",
       " 'FeatureIncrementalBottomUpLeftCornerChartParser',\n",
       " 'FeatureIncrementalChartParser',\n",
       " 'FeatureIncrementalTopDownChartParser',\n",
       " 'FeatureTopDownChartParser',\n",
       " 'FreqDist',\n",
       " 'HTTPPasswordMgrWithDefaultRealm',\n",
       " 'HeldoutProbDist',\n",
       " 'HiddenMarkovModelTagger',\n",
       " 'HiddenMarkovModelTrainer',\n",
       " 'HunposTagger',\n",
       " 'IBMModel',\n",
       " 'IBMModel1',\n",
       " 'IBMModel2',\n",
       " 'IBMModel3',\n",
       " 'IBMModel4',\n",
       " 'IBMModel5',\n",
       " 'ISRIStemmer',\n",
       " 'ImmutableMultiParentedTree',\n",
       " 'ImmutableParentedTree',\n",
       " 'ImmutableProbabilisticMixIn',\n",
       " 'ImmutableProbabilisticTree',\n",
       " 'ImmutableTree',\n",
       " 'IncrementalBottomUpChartParser',\n",
       " 'IncrementalBottomUpLeftCornerChartParser',\n",
       " 'IncrementalChartParser',\n",
       " 'IncrementalLeftCornerChartParser',\n",
       " 'IncrementalTopDownChartParser',\n",
       " 'Index',\n",
       " 'InsideChartParser',\n",
       " 'JSONTaggedDecoder',\n",
       " 'JSONTaggedEncoder',\n",
       " 'KneserNeyProbDist',\n",
       " 'LancasterStemmer',\n",
       " 'LaplaceProbDist',\n",
       " 'LazyConcatenation',\n",
       " 'LazyEnumerate',\n",
       " 'LazyIteratorList',\n",
       " 'LazyMap',\n",
       " 'LazySubsequence',\n",
       " 'LazyZip',\n",
       " 'LeftCornerChartParser',\n",
       " 'LidstoneProbDist',\n",
       " 'LineTokenizer',\n",
       " 'LogicalExpressionException',\n",
       " 'LongestChartParser',\n",
       " 'MLEProbDist',\n",
       " 'MWETokenizer',\n",
       " 'Mace',\n",
       " 'MaceCommand',\n",
       " 'MaltParser',\n",
       " 'MaxentClassifier',\n",
       " 'Model',\n",
       " 'MultiClassifierI',\n",
       " 'MultiParentedTree',\n",
       " 'MutableProbDist',\n",
       " 'NaiveBayesClassifier',\n",
       " 'NaiveBayesDependencyScorer',\n",
       " 'NgramAssocMeasures',\n",
       " 'NgramTagger',\n",
       " 'NonprojectiveDependencyParser',\n",
       " 'Nonterminal',\n",
       " 'OrderedDict',\n",
       " 'PCFG',\n",
       " 'Paice',\n",
       " 'ParallelProverBuilder',\n",
       " 'ParallelProverBuilderCommand',\n",
       " 'ParentedTree',\n",
       " 'ParserI',\n",
       " 'PerceptronTagger',\n",
       " 'PhraseTable',\n",
       " 'PorterStemmer',\n",
       " 'PositiveNaiveBayesClassifier',\n",
       " 'ProbDistI',\n",
       " 'ProbabilisticDependencyGrammar',\n",
       " 'ProbabilisticMixIn',\n",
       " 'ProbabilisticNonprojectiveParser',\n",
       " 'ProbabilisticProduction',\n",
       " 'ProbabilisticProjectiveDependencyParser',\n",
       " 'ProbabilisticTree',\n",
       " 'Production',\n",
       " 'ProjectiveDependencyParser',\n",
       " 'Prover9',\n",
       " 'Prover9Command',\n",
       " 'ProxyBasicAuthHandler',\n",
       " 'ProxyDigestAuthHandler',\n",
       " 'ProxyHandler',\n",
       " 'PunktSentenceTokenizer',\n",
       " 'QuadgramAssocMeasures',\n",
       " 'QuadgramCollocationFinder',\n",
       " 'RSLPStemmer',\n",
       " 'RTEFeatureExtractor',\n",
       " 'RUS_PICKLE',\n",
       " 'RandomChartParser',\n",
       " 'RangeFeature',\n",
       " 'ReadingCommand',\n",
       " 'RecursiveDescentParser',\n",
       " 'RegexpChunkParser',\n",
       " 'RegexpParser',\n",
       " 'RegexpStemmer',\n",
       " 'RegexpTagger',\n",
       " 'RegexpTokenizer',\n",
       " 'ReppTokenizer',\n",
       " 'ResolutionProver',\n",
       " 'ResolutionProverCommand',\n",
       " 'SExprTokenizer',\n",
       " 'SLASH',\n",
       " 'Senna',\n",
       " 'SennaChunkTagger',\n",
       " 'SennaNERTagger',\n",
       " 'SennaTagger',\n",
       " 'SequentialBackoffTagger',\n",
       " 'ShiftReduceParser',\n",
       " 'SimpleGoodTuringProbDist',\n",
       " 'SklearnClassifier',\n",
       " 'SlashFeature',\n",
       " 'SnowballStemmer',\n",
       " 'SpaceTokenizer',\n",
       " 'StackDecoder',\n",
       " 'StanfordNERTagger',\n",
       " 'StanfordPOSTagger',\n",
       " 'StanfordSegmenter',\n",
       " 'StanfordTagger',\n",
       " 'StemmerI',\n",
       " 'SteppingChartParser',\n",
       " 'SteppingRecursiveDescentParser',\n",
       " 'SteppingShiftReduceParser',\n",
       " 'SyllableTokenizer',\n",
       " 'TYPE',\n",
       " 'TabTokenizer',\n",
       " 'TableauProver',\n",
       " 'TableauProverCommand',\n",
       " 'TaggerI',\n",
       " 'TestGrammar',\n",
       " 'Text',\n",
       " 'TextCat',\n",
       " 'TextCollection',\n",
       " 'TextTilingTokenizer',\n",
       " 'TnT',\n",
       " 'TokenSearcher',\n",
       " 'ToktokTokenizer',\n",
       " 'TopDownChartParser',\n",
       " 'TransitionParser',\n",
       " 'Tree',\n",
       " 'TreebankWordTokenizer',\n",
       " 'Trie',\n",
       " 'TrigramAssocMeasures',\n",
       " 'TrigramCollocationFinder',\n",
       " 'TrigramTagger',\n",
       " 'TweetTokenizer',\n",
       " 'TypedMaxentFeatureEncoding',\n",
       " 'Undefined',\n",
       " 'UniformProbDist',\n",
       " 'UnigramTagger',\n",
       " 'UnsortedChartParser',\n",
       " 'Valuation',\n",
       " 'Variable',\n",
       " 'ViterbiParser',\n",
       " 'WekaClassifier',\n",
       " 'WhitespaceTokenizer',\n",
       " 'WittenBellProbDist',\n",
       " 'WordNetLemmatizer',\n",
       " 'WordPunctTokenizer',\n",
       " '__author__',\n",
       " '__author_email__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__classifiers__',\n",
       " '__copyright__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__keywords__',\n",
       " '__license__',\n",
       " '__loader__',\n",
       " '__longdescr__',\n",
       " '__maintainer__',\n",
       " '__maintainer_email__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__url__',\n",
       " '__version__',\n",
       " 'absolute_import',\n",
       " 'accuracy',\n",
       " 'add_logs',\n",
       " 'agreement',\n",
       " 'align',\n",
       " 'alignment_error_rate',\n",
       " 'aline',\n",
       " 'api',\n",
       " 'app',\n",
       " 'apply_features',\n",
       " 'approxrand',\n",
       " 'arity',\n",
       " 'association',\n",
       " 'bigrams',\n",
       " 'binary_distance',\n",
       " 'binary_search_file',\n",
       " 'binding_ops',\n",
       " 'bisect',\n",
       " 'blankline_tokenize',\n",
       " 'bleu',\n",
       " 'bleu_score',\n",
       " 'bllip',\n",
       " 'boolean_ops',\n",
       " 'boxer',\n",
       " 'bracket_parse',\n",
       " 'breadth_first',\n",
       " 'brill',\n",
       " 'brill_trainer',\n",
       " 'build_opener',\n",
       " 'call_megam',\n",
       " 'casual',\n",
       " 'casual_tokenize',\n",
       " 'ccg',\n",
       " 'chain',\n",
       " 'chart',\n",
       " 'chat',\n",
       " 'choose',\n",
       " 'chunk',\n",
       " 'cistem',\n",
       " 'class_types',\n",
       " 'classify',\n",
       " 'clause',\n",
       " 'clean_html',\n",
       " 'clean_url',\n",
       " 'cluster',\n",
       " 'collections',\n",
       " 'collocations',\n",
       " 'combinations',\n",
       " 'compat',\n",
       " 'config_java',\n",
       " 'config_megam',\n",
       " 'config_weka',\n",
       " 'conflicts',\n",
       " 'confusionmatrix',\n",
       " 'conllstr2tree',\n",
       " 'conlltags2tree',\n",
       " 'corenlp',\n",
       " 'corpus',\n",
       " 'crf',\n",
       " 'custom_distance',\n",
       " 'data',\n",
       " 'decisiontree',\n",
       " 'decorator',\n",
       " 'decorators',\n",
       " 'defaultdict',\n",
       " 'demo',\n",
       " 'dependencygraph',\n",
       " 'deque',\n",
       " 'discourse',\n",
       " 'distance',\n",
       " 'download',\n",
       " 'download_gui',\n",
       " 'download_shell',\n",
       " 'downloader',\n",
       " 'draw',\n",
       " 'drt',\n",
       " 'earleychart',\n",
       " 'edit_distance',\n",
       " 'edit_distance_align',\n",
       " 'elementtree_indent',\n",
       " 'entropy',\n",
       " 'equality_preds',\n",
       " 'evaluate',\n",
       " 'evaluate_sents',\n",
       " 'everygrams',\n",
       " 'extract_rels',\n",
       " 'extract_test_sentences',\n",
       " 'f_measure',\n",
       " 'featstruct',\n",
       " 'featurechart',\n",
       " 'filestring',\n",
       " 'find',\n",
       " 'flatten',\n",
       " 'fractional_presence',\n",
       " 'getproxies',\n",
       " 'ghd',\n",
       " 'glue',\n",
       " 'grammar',\n",
       " 'guess_encoding',\n",
       " 'help',\n",
       " 'hmm',\n",
       " 'hunpos',\n",
       " 'ibm1',\n",
       " 'ibm2',\n",
       " 'ibm3',\n",
       " 'ibm4',\n",
       " 'ibm5',\n",
       " 'ibm_model',\n",
       " 'ieerstr2tree',\n",
       " 'improved_close_quote_regex',\n",
       " 'improved_open_quote_regex',\n",
       " 'improved_open_single_quote_regex',\n",
       " 'improved_punct_regex',\n",
       " 'in_idle',\n",
       " 'induce_pcfg',\n",
       " 'inference',\n",
       " 'infile',\n",
       " 'inspect',\n",
       " 'install_opener',\n",
       " 'internals',\n",
       " 'interpret_sents',\n",
       " 'interval_distance',\n",
       " 'invert_dict',\n",
       " 'invert_graph',\n",
       " 'is_rel',\n",
       " 'islice',\n",
       " 'isri',\n",
       " 'jaccard_distance',\n",
       " 'json_tags',\n",
       " 'jsontags',\n",
       " 'lancaster',\n",
       " 'lazyimport',\n",
       " 'lfg',\n",
       " 'line_tokenize',\n",
       " 'linearlogic',\n",
       " 'lm',\n",
       " 'load',\n",
       " 'load_parser',\n",
       " 'locale',\n",
       " 'log_likelihood',\n",
       " 'logic',\n",
       " 'mace',\n",
       " 'malt',\n",
       " 'map_tag',\n",
       " 'mapping',\n",
       " 'masi_distance',\n",
       " 'maxent',\n",
       " 'megam',\n",
       " 'memoize',\n",
       " 'meteor',\n",
       " 'meteor_score',\n",
       " 'metrics',\n",
       " 'misc',\n",
       " 'mwe',\n",
       " 'naivebayes',\n",
       " 'ne_chunk',\n",
       " 'ne_chunk_sents',\n",
       " 'ngrams',\n",
       " 'nonprojectivedependencyparser',\n",
       " 'nonterminals',\n",
       " 'numpy',\n",
       " 'os',\n",
       " 'pad_sequence',\n",
       " 'paice',\n",
       " 'parse',\n",
       " 'parse_sents',\n",
       " 'pchart',\n",
       " 'perceptron',\n",
       " 'pk',\n",
       " 'porter',\n",
       " 'pos_tag',\n",
       " 'pos_tag_sents',\n",
       " 'positivenaivebayes',\n",
       " 'pprint',\n",
       " 'pr',\n",
       " 'precision',\n",
       " 'presence',\n",
       " 'print_function',\n",
       " 'print_string',\n",
       " 'probability',\n",
       " 'projectivedependencyparser',\n",
       " 'prover9',\n",
       " 'punkt',\n",
       " 'py25',\n",
       " 'py26',\n",
       " 'py27',\n",
       " 'pydoc',\n",
       " 'python_2_unicode_compatible',\n",
       " 'raise_unorderable_types',\n",
       " 'ranks_from_scores',\n",
       " 'ranks_from_sequence',\n",
       " 're',\n",
       " 're_show',\n",
       " 'read_grammar',\n",
       " 'read_logic',\n",
       " 'read_valuation',\n",
       " 'recall',\n",
       " 'recursivedescent',\n",
       " 'regexp',\n",
       " 'regexp_span_tokenize',\n",
       " 'regexp_tokenize',\n",
       " 'register_tag',\n",
       " 'relextract',\n",
       " 'repp',\n",
       " 'resolution',\n",
       " 'ribes',\n",
       " 'ribes_score',\n",
       " 'root_semrep',\n",
       " 'rslp',\n",
       " 'rte_classifier',\n",
       " 'rte_classify',\n",
       " 'rte_features',\n",
       " 'rtuple',\n",
       " 'scikitlearn',\n",
       " 'scores',\n",
       " 'segmentation',\n",
       " 'sem',\n",
       " 'senna',\n",
       " 'sent_tokenize',\n",
       " 'sequential',\n",
       " 'set2rel',\n",
       " 'set_proxy',\n",
       " 'sexpr',\n",
       " 'sexpr_tokenize',\n",
       " 'shiftreduce',\n",
       " 'simple',\n",
       " 'sinica_parse',\n",
       " 'skipgrams',\n",
       " 'skolemize',\n",
       " 'slice_bounds',\n",
       " 'snowball',\n",
       " 'sonority_sequencing',\n",
       " 'spearman',\n",
       " 'spearman_correlation',\n",
       " 'stack_decoder',\n",
       " 'stanford',\n",
       " 'stanford_segmenter',\n",
       " 'stem',\n",
       " 'str2tuple',\n",
       " 'string_span_tokenize',\n",
       " 'string_types',\n",
       " 'subprocess',\n",
       " 'subsumes',\n",
       " 'sum_logs',\n",
       " 'sys',\n",
       " 'tableau',\n",
       " 'tadm',\n",
       " 'tag',\n",
       " 'tagset_mapping',\n",
       " 'tagstr2tree',\n",
       " 'tbl',\n",
       " 'text',\n",
       " 'text_type',\n",
       " 'textcat',\n",
       " 'texttiling',\n",
       " 'textwrap',\n",
       " 'tkinter',\n",
       " 'tnt',\n",
       " 'tokenize',\n",
       " 'tokenwrap',\n",
       " 'toktok',\n",
       " 'toolbox',\n",
       " 'total_ordering',\n",
       " 'transitionparser',\n",
       " 'transitive_closure',\n",
       " 'translate',\n",
       " 'tree',\n",
       " 'tree2conllstr',\n",
       " 'tree2conlltags',\n",
       " 'treebank',\n",
       " 'treetransforms',\n",
       " 'trigrams',\n",
       " 'tuple2str',\n",
       " 'types',\n",
       " 'unify',\n",
       " 'unique_list',\n",
       " 'untag',\n",
       " 'usage',\n",
       " 'util',\n",
       " 'version_file',\n",
       " 'version_info',\n",
       " 'viterbi',\n",
       " 'weka',\n",
       " 'windowdiff',\n",
       " 'word_tokenize',\n",
       " 'wordnet',\n",
       " 'wordpunct_tokenize',\n",
       " 'wsd']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(nltk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    A frequency distribution for the outcomes of an experiment.  A\n",
      "    frequency distribution records the number of times each outcome of\n",
      "    an experiment has occurred.  For example, a frequency distribution\n",
      "    could be used to record the frequency of each word type in a\n",
      "    document.  Formally, a frequency distribution can be defined as a\n",
      "    function mapping from each sample to the number of times that\n",
      "    sample occurred as an outcome.\n",
      "\n",
      "    Frequency distributions are generally constructed by running a\n",
      "    number of experiments, and incrementing the count for a sample\n",
      "    every time it is an outcome of an experiment.  For example, the\n",
      "    following code will produce a frequency distribution that encodes\n",
      "    how often each word occurs in a text:\n",
      "\n",
      "        >>> from nltk.tokenize import word_tokenize\n",
      "        >>> from nltk.probability import FreqDist\n",
      "        >>> sent = 'This is an example sentence'\n",
      "        >>> fdist = FreqDist()\n",
      "        >>> for word in word_tokenize(sent):\n",
      "        ...    fdist[word.lower()] += 1\n",
      "\n",
      "    An equivalent way to do this is with the initializer:\n",
      "\n",
      "        >>> fdist = FreqDist(word.lower() for word in word_tokenize(sent))\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(nltk.FreqDist.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most common positive words: [(b'like', 138528), (b'tast', 126157), (b'good', 107583), (b'love', 106314), (b'flavor', 106291), (b'use', 103249), (b'great', 98290), (b'one', 94766), (b'product', 86413), (b'tri', 85387), (b'tea', 80631), (b'coffe', 75774), (b'make', 74686), (b'get', 71758), (b'food', 62463), (b'would', 55400), (b'time', 53612), (b'buy', 53479), (b'realli', 52432), (b'eat', 51179)]\n",
      "most_common negative words: [(b'tast', 33878), (b'like', 32139), (b'product', 27341), (b'one', 20206), (b'flavor', 18754), (b'would', 17929), (b'tri', 17642), (b'use', 15173), (b'good', 14597), (b'coffe', 14188), (b'get', 13734), (b'buy', 13563), (b'order', 12739), (b'food', 12286), (b'tea', 11254), (b'even', 11037), (b'box', 10517), (b'make', 9806), (b'time', 9580), (b'bag', 9459)]\n"
     ]
    }
   ],
   "source": [
    "freq_dist_positive=nltk.FreqDist(all_positive_words)\n",
    "freq_dist_negative=nltk.FreqDist(all_negative_words)\n",
    "print(\"most common positive words:\",freq_dist_positive.most_common(20))\n",
    "print(\"most_common negative words:\",freq_dist_negative.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bi gram,tri gram n gram\n",
    "count_vect=CountVectorizer(ngram_range=(1,2))\n",
    "final_bigram_count=count_vect.fit_transform(final['Text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 2910192)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_bigram_count.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert a collection of raw documents to a matrix of TF-IDF features.\n",
      "\n",
      "    Equivalent to :class:`CountVectorizer` followed by\n",
      "    :class:`TfidfTransformer`.\n",
      "\n",
      "    Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    input : str {'filename', 'file', 'content'}\n",
      "        If 'filename', the sequence passed as an argument to fit is\n",
      "        expected to be a list of filenames that need reading to fetch\n",
      "        the raw content to analyze.\n",
      "\n",
      "        If 'file', the sequence items must have a 'read' method (file-like\n",
      "        object) that is called to fetch the bytes in memory.\n",
      "\n",
      "        Otherwise the input is expected to be a sequence of items that\n",
      "        can be of type string or byte.\n",
      "\n",
      "    encoding : str, default='utf-8'\n",
      "        If bytes or files are given to analyze, this encoding is used to\n",
      "        decode.\n",
      "\n",
      "    decode_error : {'strict', 'ignore', 'replace'} (default='strict')\n",
      "        Instruction on what to do if a byte sequence is given to analyze that\n",
      "        contains characters not of the given `encoding`. By default, it is\n",
      "        'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
      "        values are 'ignore' and 'replace'.\n",
      "\n",
      "    strip_accents : {'ascii', 'unicode', None} (default=None)\n",
      "        Remove accents and perform other character normalization\n",
      "        during the preprocessing step.\n",
      "        'ascii' is a fast method that only works on characters that have\n",
      "        an direct ASCII mapping.\n",
      "        'unicode' is a slightly slower method that works on any characters.\n",
      "        None (default) does nothing.\n",
      "\n",
      "        Both 'ascii' and 'unicode' use NFKD normalization from\n",
      "        :func:`unicodedata.normalize`.\n",
      "\n",
      "    lowercase : bool (default=True)\n",
      "        Convert all characters to lowercase before tokenizing.\n",
      "\n",
      "    preprocessor : callable or None (default=None)\n",
      "        Override the preprocessing (string transformation) stage while\n",
      "        preserving the tokenizing and n-grams generation steps.\n",
      "        Only applies if ``analyzer is not callable``.\n",
      "\n",
      "    tokenizer : callable or None (default=None)\n",
      "        Override the string tokenization step while preserving the\n",
      "        preprocessing and n-grams generation steps.\n",
      "        Only applies if ``analyzer == 'word'``.\n",
      "\n",
      "    analyzer : str, {'word', 'char', 'char_wb'} or callable\n",
      "        Whether the feature should be made of word or character n-grams.\n",
      "        Option 'char_wb' creates character n-grams only from text inside\n",
      "        word boundaries; n-grams at the edges of words are padded with space.\n",
      "\n",
      "        If a callable is passed it is used to extract the sequence of features\n",
      "        out of the raw, unprocessed input.\n",
      "\n",
      "        .. versionchanged:: 0.21\n",
      "\n",
      "        Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
      "        first read from the file and then passed to the given callable\n",
      "        analyzer.\n",
      "\n",
      "    stop_words : str {'english'}, list, or None (default=None)\n",
      "        If a string, it is passed to _check_stop_list and the appropriate stop\n",
      "        list is returned. 'english' is currently the only supported string\n",
      "        value.\n",
      "        There are several known issues with 'english' and you should\n",
      "        consider an alternative (see :ref:`stop_words`).\n",
      "\n",
      "        If a list, that list is assumed to contain stop words, all of which\n",
      "        will be removed from the resulting tokens.\n",
      "        Only applies if ``analyzer == 'word'``.\n",
      "\n",
      "        If None, no stop words will be used. max_df can be set to a value\n",
      "        in the range [0.7, 1.0) to automatically detect and filter stop\n",
      "        words based on intra corpus document frequency of terms.\n",
      "\n",
      "    token_pattern : str\n",
      "        Regular expression denoting what constitutes a \"token\", only used\n",
      "        if ``analyzer == 'word'``. The default regexp selects tokens of 2\n",
      "        or more alphanumeric characters (punctuation is completely ignored\n",
      "        and always treated as a token separator).\n",
      "\n",
      "    ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
      "        The lower and upper boundary of the range of n-values for different\n",
      "        n-grams to be extracted. All values of n such that min_n <= n <= max_n\n",
      "        will be used. For example an ``ngram_range`` of ``(1, 1)`` means only\n",
      "        unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means\n",
      "        only bigrams.\n",
      "        Only applies if ``analyzer is not callable``.\n",
      "\n",
      "    max_df : float in range [0.0, 1.0] or int (default=1.0)\n",
      "        When building the vocabulary ignore terms that have a document\n",
      "        frequency strictly higher than the given threshold (corpus-specific\n",
      "        stop words).\n",
      "        If float, the parameter represents a proportion of documents, integer\n",
      "        absolute counts.\n",
      "        This parameter is ignored if vocabulary is not None.\n",
      "\n",
      "    min_df : float in range [0.0, 1.0] or int (default=1)\n",
      "        When building the vocabulary ignore terms that have a document\n",
      "        frequency strictly lower than the given threshold. This value is also\n",
      "        called cut-off in the literature.\n",
      "        If float, the parameter represents a proportion of documents, integer\n",
      "        absolute counts.\n",
      "        This parameter is ignored if vocabulary is not None.\n",
      "\n",
      "    max_features : int or None (default=None)\n",
      "        If not None, build a vocabulary that only consider the top\n",
      "        max_features ordered by term frequency across the corpus.\n",
      "\n",
      "        This parameter is ignored if vocabulary is not None.\n",
      "\n",
      "    vocabulary : Mapping or iterable, optional (default=None)\n",
      "        Either a Mapping (e.g., a dict) where keys are terms and values are\n",
      "        indices in the feature matrix, or an iterable over terms. If not\n",
      "        given, a vocabulary is determined from the input documents.\n",
      "\n",
      "    binary : bool (default=False)\n",
      "        If True, all non-zero term counts are set to 1. This does not mean\n",
      "        outputs will have only 0/1 values, only that the tf term in tf-idf\n",
      "        is binary. (Set idf and normalization to False to get 0/1 outputs).\n",
      "\n",
      "    dtype : type, optional (default=float64)\n",
      "        Type of the matrix returned by fit_transform() or transform().\n",
      "\n",
      "    norm : 'l1', 'l2' or None, optional (default='l2')\n",
      "        Each output row will have unit norm, either:\n",
      "        * 'l2': Sum of squares of vector elements is 1. The cosine\n",
      "        similarity between two vectors is their dot product when l2 norm has\n",
      "        been applied.\n",
      "        * 'l1': Sum of absolute values of vector elements is 1.\n",
      "        See :func:`preprocessing.normalize`.\n",
      "\n",
      "    use_idf : bool (default=True)\n",
      "        Enable inverse-document-frequency reweighting.\n",
      "\n",
      "    smooth_idf : bool (default=True)\n",
      "        Smooth idf weights by adding one to document frequencies, as if an\n",
      "        extra document was seen containing every term in the collection\n",
      "        exactly once. Prevents zero divisions.\n",
      "\n",
      "    sublinear_tf : bool (default=False)\n",
      "        Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    vocabulary_ : dict\n",
      "        A mapping of terms to feature indices.\n",
      "\n",
      "    fixed_vocabulary_: bool\n",
      "        True if a fixed vocabulary of term to indices mapping\n",
      "        is provided by the user\n",
      "\n",
      "    idf_ : array, shape (n_features)\n",
      "        The inverse document frequency (IDF) vector; only defined\n",
      "        if ``use_idf`` is True.\n",
      "\n",
      "    stop_words_ : set\n",
      "        Terms that were ignored because they either:\n",
      "\n",
      "          - occurred in too many documents (`max_df`)\n",
      "          - occurred in too few documents (`min_df`)\n",
      "          - were cut off by feature selection (`max_features`).\n",
      "\n",
      "        This is only available if no vocabulary was given.\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    CountVectorizer : Transforms text into a sparse matrix of n-gram counts.\n",
      "\n",
      "    TfidfTransformer : Performs the TF-IDF transformation from a provided\n",
      "        matrix of counts.\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    The ``stop_words_`` attribute can get large and increase the model size\n",
      "    when pickling. This attribute is provided only for introspection and can\n",
      "    be safely removed using delattr or set to None before pickling.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "    >>> corpus = [\n",
      "    ...     'This is the first document.',\n",
      "    ...     'This document is the second document.',\n",
      "    ...     'And this is the third one.',\n",
      "    ...     'Is this the first document?',\n",
      "    ... ]\n",
      "    >>> vectorizer = TfidfVectorizer()\n",
      "    >>> X = vectorizer.fit_transform(corpus)\n",
      "    >>> print(vectorizer.get_feature_names())\n",
      "    ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "    >>> print(X.shape)\n",
      "    (4, 9)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(TfidfVectorizer.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform a count matrix to a normalized tf or tf-idf representation\n",
      "\n",
      "    Tf means term-frequency while tf-idf means term-frequency times inverse\n",
      "    document-frequency. This is a common term weighting scheme in information\n",
      "    retrieval, that has also found good use in document classification.\n",
      "\n",
      "    The goal of using tf-idf instead of the raw frequencies of occurrence of a\n",
      "    token in a given document is to scale down the impact of tokens that occur\n",
      "    very frequently in a given corpus and that are hence empirically less\n",
      "    informative than features that occur in a small fraction of the training\n",
      "    corpus.\n",
      "\n",
      "    The formula that is used to compute the tf-idf for a term t of a document d\n",
      "    in a document set is tf-idf(t, d) = tf(t, d) * idf(t), and the idf is\n",
      "    computed as idf(t) = log [ n / df(t) ] + 1 (if ``smooth_idf=False``), where\n",
      "    n is the total number of documents in the document set and df(t) is the\n",
      "    document frequency of t; the document frequency is the number of documents\n",
      "    in the document set that contain the term t. The effect of adding \"1\" to\n",
      "    the idf in the equation above is that terms with zero idf, i.e., terms\n",
      "    that occur in all documents in a training set, will not be entirely\n",
      "    ignored.\n",
      "    (Note that the idf formula above differs from the standard textbook\n",
      "    notation that defines the idf as\n",
      "    idf(t) = log [ n / (df(t) + 1) ]).\n",
      "\n",
      "    If ``smooth_idf=True`` (the default), the constant \"1\" is added to the\n",
      "    numerator and denominator of the idf as if an extra document was seen\n",
      "    containing every term in the collection exactly once, which prevents\n",
      "    zero divisions: idf(d, t) = log [ (1 + n) / (1 + df(d, t)) ] + 1.\n",
      "\n",
      "    Furthermore, the formulas used to compute tf and idf depend\n",
      "    on parameter settings that correspond to the SMART notation used in IR\n",
      "    as follows:\n",
      "\n",
      "    Tf is \"n\" (natural) by default, \"l\" (logarithmic) when\n",
      "    ``sublinear_tf=True``.\n",
      "    Idf is \"t\" when use_idf is given, \"n\" (none) otherwise.\n",
      "    Normalization is \"c\" (cosine) when ``norm='l2'``, \"n\" (none)\n",
      "    when ``norm=None``.\n",
      "\n",
      "    Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    norm : 'l1', 'l2' or None, optional (default='l2')\n",
      "        Each output row will have unit norm, either:\n",
      "        * 'l2': Sum of squares of vector elements is 1. The cosine\n",
      "        similarity between two vectors is their dot product when l2 norm has\n",
      "        been applied.\n",
      "        * 'l1': Sum of absolute values of vector elements is 1.\n",
      "        See :func:`preprocessing.normalize`\n",
      "\n",
      "    use_idf : boolean (default=True)\n",
      "        Enable inverse-document-frequency reweighting.\n",
      "\n",
      "    smooth_idf : boolean (default=True)\n",
      "        Smooth idf weights by adding one to document frequencies, as if an\n",
      "        extra document was seen containing every term in the collection\n",
      "        exactly once. Prevents zero divisions.\n",
      "\n",
      "    sublinear_tf : boolean (default=False)\n",
      "        Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    idf_ : array, shape (n_features)\n",
      "        The inverse document frequency (IDF) vector; only defined\n",
      "        if  ``use_idf`` is True.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.feature_extraction.text import TfidfTransformer\n",
      "    >>> from sklearn.feature_extraction.text import CountVectorizer\n",
      "    >>> from sklearn.pipeline import Pipeline\n",
      "    >>> import numpy as np\n",
      "    >>> corpus = ['this is the first document',\n",
      "    ...           'this document is the second document',\n",
      "    ...           'and this is the third one',\n",
      "    ...           'is this the first document']\n",
      "    >>> vocabulary = ['this', 'document', 'first', 'is', 'second', 'the',\n",
      "    ...               'and', 'one']\n",
      "    >>> pipe = Pipeline([('count', CountVectorizer(vocabulary=vocabulary)),\n",
      "    ...                  ('tfid', TfidfTransformer())]).fit(corpus)\n",
      "    >>> pipe['count'].transform(corpus).toarray()\n",
      "    array([[1, 1, 1, 1, 0, 1, 0, 0],\n",
      "           [1, 2, 0, 1, 1, 1, 0, 0],\n",
      "           [1, 0, 0, 1, 0, 1, 1, 1],\n",
      "           [1, 1, 1, 1, 0, 1, 0, 0]])\n",
      "    >>> pipe['tfid'].idf_\n",
      "    array([1.        , 1.22314355, 1.51082562, 1.        , 1.91629073,\n",
      "           1.        , 1.91629073, 1.91629073])\n",
      "    >>> pipe.transform(corpus).shape\n",
      "    (4, 8)\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "\n",
      "    .. [Yates2011] R. Baeza-Yates and B. Ribeiro-Neto (2011). Modern\n",
      "                   Information Retrieval. Addison Wesley, pp. 68-74.\n",
      "\n",
      "    .. [MRS2008] C.D. Manning, P. Raghavan and H. Schütze  (2008).\n",
      "                   Introduction to Information Retrieval. Cambridge University\n",
      "                   Press, pp. 118-120.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(TfidfTransformer.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vect=TfidfVectorizer(ngram_range=(1,2))\n",
    "final_td_idf=tf_idf_vect.fit_transform(final['Text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 2910192)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_td_idf.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2910192"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature=tf_idf_vect.get_feature_names()\n",
    "len(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ales until',\n",
       " 'ales ve',\n",
       " 'ales would',\n",
       " 'ales you',\n",
       " 'alessandra',\n",
       " 'alessandra ambrosia',\n",
       " 'alessi',\n",
       " 'alessi added',\n",
       " 'alessi also',\n",
       " 'alessi and',\n",
       " 'alessi are',\n",
       " 'alessi at',\n",
       " 'alessi brand',\n",
       " 'alessi breadsticks',\n",
       " 'alessi caffe']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature[100000:100015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert a collection of raw documents to a matrix of TF-IDF features.\n",
      "\n",
      "    Equivalent to :class:`CountVectorizer` followed by\n",
      "    :class:`TfidfTransformer`.\n",
      "\n",
      "    Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    input : str {'filename', 'file', 'content'}\n",
      "        If 'filename', the sequence passed as an argument to fit is\n",
      "        expected to be a list of filenames that need reading to fetch\n",
      "        the raw content to analyze.\n",
      "\n",
      "        If 'file', the sequence items must have a 'read' method (file-like\n",
      "        object) that is called to fetch the bytes in memory.\n",
      "\n",
      "        Otherwise the input is expected to be a sequence of items that\n",
      "        can be of type string or byte.\n",
      "\n",
      "    encoding : str, default='utf-8'\n",
      "        If bytes or files are given to analyze, this encoding is used to\n",
      "        decode.\n",
      "\n",
      "    decode_error : {'strict', 'ignore', 'replace'} (default='strict')\n",
      "        Instruction on what to do if a byte sequence is given to analyze that\n",
      "        contains characters not of the given `encoding`. By default, it is\n",
      "        'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
      "        values are 'ignore' and 'replace'.\n",
      "\n",
      "    strip_accents : {'ascii', 'unicode', None} (default=None)\n",
      "        Remove accents and perform other character normalization\n",
      "        during the preprocessing step.\n",
      "        'ascii' is a fast method that only works on characters that have\n",
      "        an direct ASCII mapping.\n",
      "        'unicode' is a slightly slower method that works on any characters.\n",
      "        None (default) does nothing.\n",
      "\n",
      "        Both 'ascii' and 'unicode' use NFKD normalization from\n",
      "        :func:`unicodedata.normalize`.\n",
      "\n",
      "    lowercase : bool (default=True)\n",
      "        Convert all characters to lowercase before tokenizing.\n",
      "\n",
      "    preprocessor : callable or None (default=None)\n",
      "        Override the preprocessing (string transformation) stage while\n",
      "        preserving the tokenizing and n-grams generation steps.\n",
      "        Only applies if ``analyzer is not callable``.\n",
      "\n",
      "    tokenizer : callable or None (default=None)\n",
      "        Override the string tokenization step while preserving the\n",
      "        preprocessing and n-grams generation steps.\n",
      "        Only applies if ``analyzer == 'word'``.\n",
      "\n",
      "    analyzer : str, {'word', 'char', 'char_wb'} or callable\n",
      "        Whether the feature should be made of word or character n-grams.\n",
      "        Option 'char_wb' creates character n-grams only from text inside\n",
      "        word boundaries; n-grams at the edges of words are padded with space.\n",
      "\n",
      "        If a callable is passed it is used to extract the sequence of features\n",
      "        out of the raw, unprocessed input.\n",
      "\n",
      "        .. versionchanged:: 0.21\n",
      "\n",
      "        Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
      "        first read from the file and then passed to the given callable\n",
      "        analyzer.\n",
      "\n",
      "    stop_words : str {'english'}, list, or None (default=None)\n",
      "        If a string, it is passed to _check_stop_list and the appropriate stop\n",
      "        list is returned. 'english' is currently the only supported string\n",
      "        value.\n",
      "        There are several known issues with 'english' and you should\n",
      "        consider an alternative (see :ref:`stop_words`).\n",
      "\n",
      "        If a list, that list is assumed to contain stop words, all of which\n",
      "        will be removed from the resulting tokens.\n",
      "        Only applies if ``analyzer == 'word'``.\n",
      "\n",
      "        If None, no stop words will be used. max_df can be set to a value\n",
      "        in the range [0.7, 1.0) to automatically detect and filter stop\n",
      "        words based on intra corpus document frequency of terms.\n",
      "\n",
      "    token_pattern : str\n",
      "        Regular expression denoting what constitutes a \"token\", only used\n",
      "        if ``analyzer == 'word'``. The default regexp selects tokens of 2\n",
      "        or more alphanumeric characters (punctuation is completely ignored\n",
      "        and always treated as a token separator).\n",
      "\n",
      "    ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
      "        The lower and upper boundary of the range of n-values for different\n",
      "        n-grams to be extracted. All values of n such that min_n <= n <= max_n\n",
      "        will be used. For example an ``ngram_range`` of ``(1, 1)`` means only\n",
      "        unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means\n",
      "        only bigrams.\n",
      "        Only applies if ``analyzer is not callable``.\n",
      "\n",
      "    max_df : float in range [0.0, 1.0] or int (default=1.0)\n",
      "        When building the vocabulary ignore terms that have a document\n",
      "        frequency strictly higher than the given threshold (corpus-specific\n",
      "        stop words).\n",
      "        If float, the parameter represents a proportion of documents, integer\n",
      "        absolute counts.\n",
      "        This parameter is ignored if vocabulary is not None.\n",
      "\n",
      "    min_df : float in range [0.0, 1.0] or int (default=1)\n",
      "        When building the vocabulary ignore terms that have a document\n",
      "        frequency strictly lower than the given threshold. This value is also\n",
      "        called cut-off in the literature.\n",
      "        If float, the parameter represents a proportion of documents, integer\n",
      "        absolute counts.\n",
      "        This parameter is ignored if vocabulary is not None.\n",
      "\n",
      "    max_features : int or None (default=None)\n",
      "        If not None, build a vocabulary that only consider the top\n",
      "        max_features ordered by term frequency across the corpus.\n",
      "\n",
      "        This parameter is ignored if vocabulary is not None.\n",
      "\n",
      "    vocabulary : Mapping or iterable, optional (default=None)\n",
      "        Either a Mapping (e.g., a dict) where keys are terms and values are\n",
      "        indices in the feature matrix, or an iterable over terms. If not\n",
      "        given, a vocabulary is determined from the input documents.\n",
      "\n",
      "    binary : bool (default=False)\n",
      "        If True, all non-zero term counts are set to 1. This does not mean\n",
      "        outputs will have only 0/1 values, only that the tf term in tf-idf\n",
      "        is binary. (Set idf and normalization to False to get 0/1 outputs).\n",
      "\n",
      "    dtype : type, optional (default=float64)\n",
      "        Type of the matrix returned by fit_transform() or transform().\n",
      "\n",
      "    norm : 'l1', 'l2' or None, optional (default='l2')\n",
      "        Each output row will have unit norm, either:\n",
      "        * 'l2': Sum of squares of vector elements is 1. The cosine\n",
      "        similarity between two vectors is their dot product when l2 norm has\n",
      "        been applied.\n",
      "        * 'l1': Sum of absolute values of vector elements is 1.\n",
      "        See :func:`preprocessing.normalize`.\n",
      "\n",
      "    use_idf : bool (default=True)\n",
      "        Enable inverse-document-frequency reweighting.\n",
      "\n",
      "    smooth_idf : bool (default=True)\n",
      "        Smooth idf weights by adding one to document frequencies, as if an\n",
      "        extra document was seen containing every term in the collection\n",
      "        exactly once. Prevents zero divisions.\n",
      "\n",
      "    sublinear_tf : bool (default=False)\n",
      "        Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    vocabulary_ : dict\n",
      "        A mapping of terms to feature indices.\n",
      "\n",
      "    fixed_vocabulary_: bool\n",
      "        True if a fixed vocabulary of term to indices mapping\n",
      "        is provided by the user\n",
      "\n",
      "    idf_ : array, shape (n_features)\n",
      "        The inverse document frequency (IDF) vector; only defined\n",
      "        if ``use_idf`` is True.\n",
      "\n",
      "    stop_words_ : set\n",
      "        Terms that were ignored because they either:\n",
      "\n",
      "          - occurred in too many documents (`max_df`)\n",
      "          - occurred in too few documents (`min_df`)\n",
      "          - were cut off by feature selection (`max_features`).\n",
      "\n",
      "        This is only available if no vocabulary was given.\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    CountVectorizer : Transforms text into a sparse matrix of n-gram counts.\n",
      "\n",
      "    TfidfTransformer : Performs the TF-IDF transformation from a provided\n",
      "        matrix of counts.\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    The ``stop_words_`` attribute can get large and increase the model size\n",
      "    when pickling. This attribute is provided only for introspection and can\n",
      "    be safely removed using delattr or set to None before pickling.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "    >>> corpus = [\n",
      "    ...     'This is the first document.',\n",
      "    ...     'This document is the second document.',\n",
      "    ...     'And this is the third one.',\n",
      "    ...     'Is this the first document?',\n",
      "    ... ]\n",
      "    >>> vectorizer = TfidfVectorizer()\n",
      "    >>> X = vectorizer.fit_transform(corpus)\n",
      "    >>> print(vectorizer.get_feature_names())\n",
      "    ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "    >>> print(X.shape)\n",
      "    (4, 9)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(tf_idf_vect.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(364171, 2910192)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(final_td_idf)\n",
    "print(final_td_idf[3,:].toarray()[0])#list of array\n",
    "final_td_idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_tfidf_feats(row, features, top_n=25):\n",
    "    '''get top n tfidf values in row and return them with their corresponding'''\n",
    "    topn_ids= np.argsort(row)[::-1][:top_n]#this function sort the data ince nd then change to dec and select top 25 val\n",
    "    top_feats=[(feature[i],row[i]) for i in topn_ids]\n",
    "    df=pd.DataFrame(top_feats)\n",
    "    df.columns=['features','tfidf']\n",
    "    return df\n",
    "top_ifidf=top_tfidf_feats(final_td_idf[1,:].toarray()[0],feature,25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>as jumbo</td>\n",
       "      <td>0.390489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jumbo</td>\n",
       "      <td>0.260971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>unsalted not</td>\n",
       "      <td>0.201475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jumbo salted</td>\n",
       "      <td>0.201475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vendor intended</td>\n",
       "      <td>0.201475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sized unsalted</td>\n",
       "      <td>0.201475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>arrived labeled</td>\n",
       "      <td>0.187395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>peanuts</td>\n",
       "      <td>0.186777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>actually small</td>\n",
       "      <td>0.184594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>error or</td>\n",
       "      <td>0.176745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>represent the</td>\n",
       "      <td>0.162063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>to represent</td>\n",
       "      <td>0.161483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>salted peanuts</td>\n",
       "      <td>0.157496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>small sized</td>\n",
       "      <td>0.154333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>peanuts were</td>\n",
       "      <td>0.150833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>peanuts the</td>\n",
       "      <td>0.149735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>an error</td>\n",
       "      <td>0.146627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>represent</td>\n",
       "      <td>0.140615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>were actually</td>\n",
       "      <td>0.133485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>the peanuts</td>\n",
       "      <td>0.127286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>labeled as</td>\n",
       "      <td>0.127042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>intended to</td>\n",
       "      <td>0.126160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>unsalted</td>\n",
       "      <td>0.122116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>the vendor</td>\n",
       "      <td>0.117553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>error</td>\n",
       "      <td>0.116271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           features     tfidf\n",
       "0          as jumbo  0.390489\n",
       "1             jumbo  0.260971\n",
       "2      unsalted not  0.201475\n",
       "3      jumbo salted  0.201475\n",
       "4   vendor intended  0.201475\n",
       "5    sized unsalted  0.201475\n",
       "6   arrived labeled  0.187395\n",
       "7           peanuts  0.186777\n",
       "8    actually small  0.184594\n",
       "9          error or  0.176745\n",
       "10    represent the  0.162063\n",
       "11     to represent  0.161483\n",
       "12   salted peanuts  0.157496\n",
       "13      small sized  0.154333\n",
       "14     peanuts were  0.150833\n",
       "15      peanuts the  0.149735\n",
       "16         an error  0.146627\n",
       "17        represent  0.140615\n",
       "18    were actually  0.133485\n",
       "19      the peanuts  0.127286\n",
       "20       labeled as  0.127042\n",
       "21      intended to  0.126160\n",
       "22         unsalted  0.122116\n",
       "23       the vendor  0.117553\n",
       "24            error  0.116271"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_ifidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 208746, 1366174, 2700457, 1366261, 2741354, 2270266,  202002,\n",
       "       1881462,   67006,  860270, 2105227, 2616754, 2162281, 2283613,\n",
       "       1881968, 1881910,  136880, 2105180, 2803448, 2530577, 1401864,\n",
       "       1305606, 2700388, 2538834,  860179])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(final_td_idf[1,:].toarray()[0])[::-1][:25]\n",
    "#this function sort the data incen then change to dec and select top 25 val\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  WORD2VE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train, use and evaluate neural networks described in https://code.google.com/p/word2vec/.\n",
      "\n",
      "    Once you're finished training a model (=no more updates, only querying)\n",
      "    store and use only the :class:`~gensim.models.keyedvectors.KeyedVectors` instance in `self.wv` to reduce memory.\n",
      "\n",
      "    The model can be stored/loaded via its :meth:`~gensim.models.word2vec.Word2Vec.save` and\n",
      "    :meth:`~gensim.models.word2vec.Word2Vec.load` methods.\n",
      "\n",
      "    The trained word vectors can also be stored/loaded from a format compatible with the\n",
      "    original word2vec implementation via `self.wv.save_word2vec_format`\n",
      "    and :meth:`gensim.models.keyedvectors.KeyedVectors.load_word2vec_format`.\n",
      "\n",
      "    Some important attributes are the following:\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    wv : :class:`~gensim.models.keyedvectors.Word2VecKeyedVectors`\n",
      "        This object essentially contains the mapping between words and embeddings. After training, it can be used\n",
      "        directly to query those embeddings in various ways. See the module level docstring for examples.\n",
      "\n",
      "    vocabulary : :class:`~gensim.models.word2vec.Word2VecVocab`\n",
      "        This object represents the vocabulary (sometimes called Dictionary in gensim) of the model.\n",
      "        Besides keeping track of all unique words, this object provides extra functionality, such as\n",
      "        constructing a huffman tree (frequent words are closer to the root), or discarding extremely rare words.\n",
      "\n",
      "    trainables : :class:`~gensim.models.word2vec.Word2VecTrainables`\n",
      "        This object represents the inner shallow neural network used to train the embeddings. The semantics of the\n",
      "        network differ slightly in the two available training modes (CBOW or SG) but you can think of it as a NN with\n",
      "        a single projection and hidden layer which we train on the corpus. The weights are then used as our embeddings\n",
      "        (which means that the size of the hidden layer is equal to the number of features `self.size`).\n",
      "\n",
      "    \n",
      "Mapping between words and vectors for the :class:`~gensim.models.Word2Vec` model.\n",
      "    Used to perform operations on the vectors such as vector lookup, distance, similarity etc.\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(Word2Vec.__doc__)\n",
    "print(KeyedVectors.__doc__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\",binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.07421875e-01, -2.01171875e-01,  1.23046875e-01,  2.11914062e-01,\n",
       "       -9.13085938e-02,  2.16796875e-01, -1.31835938e-01,  8.30078125e-02,\n",
       "        2.02148438e-01,  4.78515625e-02,  3.66210938e-02, -2.45361328e-02,\n",
       "        2.39257812e-02, -1.60156250e-01, -2.61230469e-02,  9.71679688e-02,\n",
       "       -6.34765625e-02,  1.84570312e-01,  1.70898438e-01, -1.63085938e-01,\n",
       "       -1.09375000e-01,  1.49414062e-01, -4.65393066e-04,  9.61914062e-02,\n",
       "        1.68945312e-01,  2.60925293e-03,  8.93554688e-02,  6.49414062e-02,\n",
       "        3.56445312e-02, -6.93359375e-02, -1.46484375e-01, -1.21093750e-01,\n",
       "       -2.27539062e-01,  2.45361328e-02, -1.24511719e-01, -3.18359375e-01,\n",
       "       -2.20703125e-01,  1.30859375e-01,  3.66210938e-02, -3.63769531e-02,\n",
       "       -1.13281250e-01,  1.95312500e-01,  9.76562500e-02,  1.26953125e-01,\n",
       "        6.59179688e-02,  6.93359375e-02,  1.02539062e-02,  1.75781250e-01,\n",
       "       -1.68945312e-01,  1.21307373e-03, -2.98828125e-01, -1.15234375e-01,\n",
       "        5.66406250e-02, -1.77734375e-01, -2.08984375e-01,  1.76757812e-01,\n",
       "        2.38037109e-02, -2.57812500e-01, -4.46777344e-02,  1.88476562e-01,\n",
       "        5.51757812e-02,  5.02929688e-02, -1.06933594e-01,  1.89453125e-01,\n",
       "       -1.16210938e-01,  8.49609375e-02, -1.71875000e-01,  2.45117188e-01,\n",
       "       -1.73828125e-01, -8.30078125e-03,  4.56542969e-02, -1.61132812e-02,\n",
       "        1.86523438e-01, -6.05468750e-02, -4.17480469e-02,  1.82617188e-01,\n",
       "        2.20703125e-01, -1.22558594e-01, -2.55126953e-02, -3.08593750e-01,\n",
       "        9.13085938e-02,  1.60156250e-01,  1.70898438e-01,  1.19628906e-01,\n",
       "        7.08007812e-02, -2.64892578e-02, -3.08837891e-02,  4.06250000e-01,\n",
       "       -1.01562500e-01,  5.71289062e-02, -7.26318359e-03, -9.17968750e-02,\n",
       "       -1.50390625e-01, -2.55859375e-01,  2.16796875e-01, -3.63769531e-02,\n",
       "        2.24609375e-01,  8.00781250e-02,  1.56250000e-01,  5.27343750e-02,\n",
       "        1.50390625e-01, -1.14746094e-01, -8.64257812e-02,  1.19140625e-01,\n",
       "       -7.17773438e-02,  2.73437500e-01, -1.64062500e-01,  7.29370117e-03,\n",
       "        4.21875000e-01, -1.12792969e-01, -1.35742188e-01, -1.31835938e-01,\n",
       "       -1.37695312e-01, -7.66601562e-02,  6.25000000e-02,  4.98046875e-02,\n",
       "       -1.91406250e-01, -6.03027344e-02,  2.27539062e-01,  5.88378906e-02,\n",
       "       -3.24218750e-01,  5.41992188e-02, -1.35742188e-01,  8.17871094e-03,\n",
       "       -5.24902344e-02, -1.74713135e-03, -9.81445312e-02, -2.86865234e-02,\n",
       "        3.61328125e-02,  2.15820312e-01,  5.98144531e-02, -3.08593750e-01,\n",
       "       -2.27539062e-01,  2.61718750e-01,  9.86328125e-02, -5.07812500e-02,\n",
       "        1.78222656e-02,  1.31835938e-01, -5.35156250e-01, -1.81640625e-01,\n",
       "        1.38671875e-01, -3.10546875e-01, -9.71679688e-02,  1.31835938e-01,\n",
       "       -1.16210938e-01,  7.03125000e-02,  2.85156250e-01,  3.51562500e-02,\n",
       "       -1.01562500e-01, -3.75976562e-02,  1.41601562e-01,  1.42578125e-01,\n",
       "       -5.68847656e-02,  2.65625000e-01, -2.09960938e-01,  9.64355469e-03,\n",
       "       -6.68945312e-02, -4.83398438e-02, -6.10351562e-02,  2.45117188e-01,\n",
       "       -9.66796875e-02,  1.78222656e-02, -1.27929688e-01, -4.78515625e-02,\n",
       "       -7.26318359e-03,  1.79687500e-01,  2.78320312e-02, -2.10937500e-01,\n",
       "       -1.43554688e-01, -1.27929688e-01,  1.73339844e-02, -3.60107422e-03,\n",
       "       -2.04101562e-01,  3.63159180e-03, -1.19628906e-01, -6.15234375e-02,\n",
       "        5.93261719e-02, -3.23486328e-03, -1.70898438e-01, -3.14941406e-02,\n",
       "       -8.88671875e-02, -2.89062500e-01,  3.44238281e-02, -1.87500000e-01,\n",
       "        2.94921875e-01,  1.58203125e-01, -1.19628906e-01,  7.61718750e-02,\n",
       "        6.39648438e-02, -4.68750000e-02, -6.83593750e-02,  1.21459961e-02,\n",
       "       -1.44531250e-01,  4.54101562e-02,  3.68652344e-02,  3.88671875e-01,\n",
       "        1.45507812e-01, -2.55859375e-01, -4.46777344e-02, -1.33789062e-01,\n",
       "       -1.38671875e-01,  6.59179688e-02,  1.37695312e-01,  1.14746094e-01,\n",
       "        2.03125000e-01, -4.78515625e-02,  1.80664062e-02, -8.54492188e-02,\n",
       "       -2.48046875e-01, -3.39843750e-01, -2.83203125e-02,  1.05468750e-01,\n",
       "       -2.14843750e-01, -8.74023438e-02,  7.12890625e-02,  1.87500000e-01,\n",
       "       -1.12304688e-01,  2.73437500e-01, -3.26171875e-01, -1.77734375e-01,\n",
       "       -4.24804688e-02, -2.69531250e-01,  6.64062500e-02, -6.88476562e-02,\n",
       "       -1.99218750e-01, -7.03125000e-02, -2.43164062e-01, -3.66210938e-02,\n",
       "       -7.37304688e-02, -1.77734375e-01,  9.17968750e-02, -1.25000000e-01,\n",
       "       -1.65039062e-01, -3.57421875e-01, -2.85156250e-01, -1.66992188e-01,\n",
       "        1.97265625e-01, -1.53320312e-01,  2.31933594e-02,  2.06054688e-01,\n",
       "        1.80664062e-01, -2.74658203e-02, -1.92382812e-01, -9.61914062e-02,\n",
       "       -1.06811523e-02, -4.73632812e-02,  6.54296875e-02, -1.25732422e-02,\n",
       "        1.78222656e-02, -8.00781250e-02, -2.59765625e-01,  9.37500000e-02,\n",
       "       -7.81250000e-02,  4.68750000e-02, -2.22167969e-02,  1.86767578e-02,\n",
       "        3.11279297e-02,  1.04980469e-02, -1.69921875e-01,  2.58789062e-02,\n",
       "       -3.41796875e-02, -1.44042969e-02, -5.46875000e-02, -8.78906250e-02,\n",
       "        1.96838379e-03,  2.23632812e-01, -1.36718750e-01,  1.75781250e-01,\n",
       "       -1.63085938e-01,  1.87500000e-01,  3.44238281e-02, -5.63964844e-02,\n",
       "       -2.27689743e-05,  4.27246094e-02,  5.81054688e-02, -1.07910156e-01,\n",
       "       -3.88183594e-02, -2.69531250e-01,  3.34472656e-02,  9.81445312e-02,\n",
       "        5.63964844e-02,  2.23632812e-01, -5.49316406e-02,  1.46484375e-01,\n",
       "        5.93261719e-02, -2.19726562e-01,  6.39648438e-02,  1.66015625e-02,\n",
       "        4.56542969e-02,  3.26171875e-01, -3.80859375e-01,  1.70898438e-01,\n",
       "        5.66406250e-02, -1.04492188e-01,  1.38671875e-01, -1.57226562e-01,\n",
       "        3.23486328e-03, -4.80957031e-02, -2.48046875e-01, -6.20117188e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['computer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-647fd66343c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'women'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.wv.most_similar('women')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('tasti')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('tasty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.wv.similarity('man','women')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity('tasty','taste')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train own word2vec\n",
    "\n",
    "i=0\n",
    "list_of_sent=[]\n",
    "for sent in final['Text'].values:\n",
    "    filtered_sentence=[]\n",
    "    sent=cleanhtml(sent)\n",
    "    for w in sent.split():\n",
    "        for cleaned_word in cleanpunc(w).split():\n",
    "            if(cleaned_word.isalpha()):\n",
    "                filtered_sentence.append(cleaned_word.lower())\n",
    "            else:\n",
    "                continue\n",
    "    list_of_sent.append(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w2v_model=Word2Vec(list_of_sent,min_count=5,size=50,workers=4)\n",
    "print(w2v_model.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=list(w2v_model.wv.vocab)\n",
    "print(len(words))\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar('tasty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar('like')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  AVG W2V, TFIDF-W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVERAGE WORD2VEC\n",
    "#COMPUTE AVERAGE WORD2VEC FOR EACH REVIEW\n",
    "sent_vectors=[];\n",
    "for sent in list_of_sent:\n",
    "    sent_vec=np.zeros(50)\n",
    "    #print(sent_vec)\n",
    "    cnt_word=0\n",
    "    for word in sent:\n",
    "        try:\n",
    "            vec=w2v_model.ww[word]\n",
    "            sent_vec+=vec\n",
    "            cnt_word+=1\n",
    "        except:\n",
    "            pass\n",
    "    sent_vec/=cnt_word\n",
    "    sent_vectors.append(sent_vec)\n",
    "print(len(sent_vectors))\n",
    "print(len(sent_vectors[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF weighted word2vec\n",
    "tfidf_feat=tf_idf_vect.get_feature_names()\n",
    "tfidf_Sent_vectors=[]\n",
    "row=0;\n",
    "for sent in list_of_sent:\n",
    "    sent_vec=np.zeros(50)\n",
    "    weight_sum=0;\n",
    "    for word in sent:\n",
    "        try:\n",
    "            vec=w2v_model.wv[word]# this will return vector corresponding to word\n",
    "            tfidf=final_td_idf[rows,tfidf_feat.index(word)]#this will extract the tfidf value correspong to word \n",
    "            sent_vec+= (vec*tfidf)\n",
    "            weight_sum+=tf_idf\n",
    "        except:\n",
    "            pass\n",
    "    sent_vec/=weight_sum\n",
    "    tfidf_Sent_vectors.append(sent_vec)\n",
    "    row+=1\n",
    "print(len(sent_vectors))\n",
    "print(len(sent_vectors[0]))\n",
    "                               \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
